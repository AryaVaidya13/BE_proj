[
  {
    "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
    "url": "https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
    "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "year": 2023,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.12597",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "49299019",
        "name": "Junnan Li"
      },
      {
        "authorId": "2981509",
        "name": "Dongxu Li"
      },
      {
        "authorId": "1702137",
        "name": "S. Savarese"
      },
      {
        "authorId": "2184854289",
        "name": "Steven C. H. Hoi"
      }
    ],
    "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
  },
  {
    "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
    "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "year": 2022,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.11903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "119640649",
        "name": "Jason Wei"
      },
      {
        "authorId": "1524732527",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "1714772",
        "name": "Dale Schuurmans"
      },
      {
        "authorId": "40377863",
        "name": "Maarten Bosma"
      },
      {
        "authorId": "2226805",
        "name": "Ed H. Chi"
      },
      {
        "authorId": "144956443",
        "name": "F. Xia"
      },
      {
        "authorId": "1998340269",
        "name": "Quoc Le"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      }
    ],
    "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
  },
  {
    "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "url": "https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "year": 2021,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.09685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2157840220",
        "name": "J. E. Hu"
      },
      {
        "authorId": "1752875",
        "name": "Yelong Shen"
      },
      {
        "authorId": "104100507",
        "name": "Phillip Wallis"
      },
      {
        "authorId": "1388725932",
        "name": "Zeyuan Allen-Zhu"
      },
      {
        "authorId": "2110486765",
        "name": "Yuanzhi Li"
      },
      {
        "authorId": "2135571585",
        "name": "Shean Wang"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ],
    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
  },
  {
    "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
    "url": "https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
    "title": "Evaluating Large Language Models Trained on Code",
    "year": 2021,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.03374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2108828435",
        "name": "Mark Chen"
      },
      {
        "authorId": "2065005836",
        "name": "Jerry Tworek"
      },
      {
        "authorId": "35450887",
        "name": "Heewoo Jun"
      },
      {
        "authorId": "153930486",
        "name": "Qiming Yuan"
      },
      {
        "authorId": "2117715024",
        "name": "Henrique Pond√©"
      },
      {
        "authorId": "2053807409",
        "name": "Jared Kaplan"
      },
      {
        "authorId": "144632352",
        "name": "Harrison Edwards"
      },
      {
        "authorId": "51178856",
        "name": "Yura Burda"
      },
      {
        "authorId": "2117706920",
        "name": "Nicholas Joseph"
      },
      {
        "authorId": "2065151121",
        "name": "Greg Brockman"
      },
      {
        "authorId": "2064770039",
        "name": "Alex Ray"
      },
      {
        "authorId": "41158993",
        "name": "Raul Puri"
      },
      {
        "authorId": "2064404342",
        "name": "Gretchen Krueger"
      },
      {
        "authorId": "2136008481",
        "name": "Michael Petrov"
      },
      {
        "authorId": "2103414",
        "name": "Heidy Khlaaf"
      },
      {
        "authorId": "144864359",
        "name": "Girish Sastry"
      },
      {
        "authorId": "2051714782",
        "name": "Pamela Mishkin"
      },
      {
        "authorId": "1466431052",
        "name": "Brooke Chan"
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray"
      },
      {
        "authorId": "39849748",
        "name": "Nick Ryder"
      },
      {
        "authorId": "2068123790",
        "name": "Mikhail Pavlov"
      },
      {
        "authorId": "146162186",
        "name": "Alethea Power"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "2275251620",
        "name": "Mo Bavarian"
      },
      {
        "authorId": "2059411355",
        "name": "Clemens Winter"
      },
      {
        "authorId": "2275252092",
        "name": "P. Tillet"
      },
      {
        "authorId": "9927844",
        "name": "F. Such"
      },
      {
        "authorId": "80876468",
        "name": "D. Cummings"
      },
      {
        "authorId": "3407285",
        "name": "Matthias Plappert"
      },
      {
        "authorId": "2117714459",
        "name": "Fotios Chantzis"
      },
      {
        "authorId": "2057742918",
        "name": "Elizabeth Barnes"
      },
      {
        "authorId": "1404060687",
        "name": "Ariel Herbert-Voss"
      },
      {
        "authorId": "39121861",
        "name": "William H. Guss"
      },
      {
        "authorId": "38967461",
        "name": "Alex Nichol"
      },
      {
        "authorId": "2256699302",
        "name": "Igor Babuschkin"
      },
      {
        "authorId": "2054519183",
        "name": "S. Balaji"
      },
      {
        "authorId": "150298413",
        "name": "Shantanu Jain"
      },
      {
        "authorId": "153480842",
        "name": "A. Carr"
      },
      {
        "authorId": "2990741",
        "name": "Jan Leike"
      },
      {
        "authorId": "2318703608",
        "name": "Josh Achiam"
      },
      {
        "authorId": "40055795",
        "name": "Vedant Misra"
      },
      {
        "authorId": "1404556973",
        "name": "Evan Morikawa"
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford"
      },
      {
        "authorId": "3555117",
        "name": "M. Knight"
      },
      {
        "authorId": "35167962",
        "name": "Miles Brundage"
      },
      {
        "authorId": "2117715631",
        "name": "Mira Murati"
      },
      {
        "authorId": "2059169400",
        "name": "Katie Mayer"
      },
      {
        "authorId": "2930640",
        "name": "Peter Welinder"
      },
      {
        "authorId": "39593364",
        "name": "Bob McGrew"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      },
      {
        "authorId": "52238703",
        "name": "Sam McCandlish"
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever"
      },
      {
        "authorId": "2563432",
        "name": "Wojciech Zaremba"
      }
    ],
    "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics."
  },
  {
    "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
    "url": "https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda",
    "title": "Large Language Models are Zero-Shot Reasoners",
    "year": 2022,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.11916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2081836120",
        "name": "Takeshi Kojima"
      },
      {
        "authorId": "2046135",
        "name": "S. Gu"
      },
      {
        "authorId": "1557386977",
        "name": "Machel Reid"
      },
      {
        "authorId": "2153732825",
        "name": "Yutaka Matsuo"
      },
      {
        "authorId": "1715282",
        "name": "Yusuke Iwasawa"
      }
    ],
    "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
  },
  {
    "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b",
    "url": "https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b",
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "year": 2023,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.10592",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.10592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1388731230",
        "name": "Deyao Zhu"
      },
      {
        "authorId": "2153417252",
        "name": "Jun Chen"
      },
      {
        "authorId": "2151708219",
        "name": "Xiaoqian Shen"
      },
      {
        "authorId": "2144440192",
        "name": "Xiang Li"
      },
      {
        "authorId": "1712479",
        "name": "Mohamed Elhoseiny"
      }
    ],
    "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/."
  },
  {
    "paperId": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
    "url": "https://www.semanticscholar.org/paper/f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
    "title": "A Survey of Large Language Models",
    "year": 2023,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.18223",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.18223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "1997234792",
        "name": "Tianyi Tang"
      },
      {
        "authorId": "72541556",
        "name": "Xiaolei Wang"
      },
      {
        "authorId": "151472453",
        "name": "Yupeng Hou"
      },
      {
        "authorId": "2007666579",
        "name": "Yingqian Min"
      },
      {
        "authorId": "2107926615",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "2155570461",
        "name": "Junjie Zhang"
      },
      {
        "authorId": "2198280871",
        "name": "Zican Dong"
      },
      {
        "authorId": "2111895473",
        "name": "Yifan Du"
      },
      {
        "authorId": "2181967397",
        "name": "Chen Yang"
      },
      {
        "authorId": "2109315001",
        "name": "Yushuo Chen"
      },
      {
        "authorId": "46842323",
        "name": "Z. Chen"
      },
      {
        "authorId": "2118240359",
        "name": "Jinhao Jiang"
      },
      {
        "authorId": "1708171825",
        "name": "Ruiyang Ren"
      },
      {
        "authorId": "2209136299",
        "name": "Yifan Li"
      },
      {
        "authorId": "2109887979",
        "name": "Xinyu Tang"
      },
      {
        "authorId": "2119618242",
        "name": "Zikang Liu"
      },
      {
        "authorId": "2108129670",
        "name": "Peiyu Liu"
      },
      {
        "authorId": "50204644",
        "name": "J. Nie"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions."
  },
  {
    "paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "url": "https://www.semanticscholar.org/paper/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
    "year": 2023,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2280046531",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2275320371",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2275341478",
        "name": "Xinyu Gao"
      },
      {
        "authorId": "2275191447",
        "name": "Kangxiang Jia"
      },
      {
        "authorId": "2275530552",
        "name": "Jinliu Pan"
      },
      {
        "authorId": "2275171009",
        "name": "Yuxi Bi"
      },
      {
        "authorId": "2276187454",
        "name": "Yi Dai"
      },
      {
        "authorId": "2275540959",
        "name": "Jiawei Sun"
      },
      {
        "authorId": "2258800561",
        "name": "Qianyu Guo"
      },
      {
        "authorId": "2291409458",
        "name": "Meng Wang"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      }
    ],
    "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development."
  },
  {
    "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
    "url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
    "title": "Program Synthesis with Large Language Models",
    "year": 2021,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.07732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2058365883",
        "name": "Jacob Austin"
      },
      {
        "authorId": "2624088",
        "name": "Augustus Odena"
      },
      {
        "authorId": "51150953",
        "name": "Maxwell Nye"
      },
      {
        "authorId": "40377863",
        "name": "Maarten Bosma"
      },
      {
        "authorId": "47407464",
        "name": "H. Michalewski"
      },
      {
        "authorId": "35363891",
        "name": "David Dohan"
      },
      {
        "authorId": "122064392",
        "name": "Ellen Jiang"
      },
      {
        "authorId": "145941081",
        "name": "Carrie J. Cai"
      },
      {
        "authorId": "2053829286",
        "name": "Michael Terry"
      },
      {
        "authorId": "1397917613",
        "name": "Quoc V. Le"
      },
      {
        "authorId": "152549864",
        "name": "Charles Sutton"
      }
    ],
    "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input."
  },
  {
    "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
    "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "year": 2023,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.10601",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2093302161",
        "name": "Shunyu Yao"
      },
      {
        "authorId": "150978762",
        "name": "Dian Yu"
      },
      {
        "authorId": "2144551262",
        "name": "Jeffrey Zhao"
      },
      {
        "authorId": "1697494",
        "name": "Izhak Shafran"
      },
      {
        "authorId": "1799860",
        "name": "T. Griffiths"
      },
      {
        "authorId": "145144022",
        "name": "Yuan Cao"
      },
      {
        "authorId": "144958935",
        "name": "Karthik Narasimhan"
      }
    ],
    "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm."
  }
]